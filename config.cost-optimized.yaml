# Cost-Optimized Configuration
# Maximizes use of free models while maintaining quality

server:
  host: "0.0.0.0"
  port: "8080"
  app_name: "LLM Router-CostOpt"
  referer_url: "https://llm-router.ai"
  cors_origins: ["*"]

llm_provider:
  api_key: "${OPENROUTER_API_KEY}"
  base_url: "https://openrouter.ai/api/v1"
  allowed_models:
    # Only free models for maximum cost efficiency
    - "meta-llama/llama-3.2-3b-instruct:free"
    - "meta-llama/llama-3.2-1b-instruct:free"
    - "google/gemma-2-9b-it:free"
    - "microsoft/phi-3-mini-128k-instruct:free"
    - "microsoft/phi-3-medium-128k-instruct:free"
    - "qwen/qwen-2-7b-instruct:free"
    - "huggingfaceh4/zephyr-7b-beta:free"
    - "openchat/openchat-7b:free"

bandit:
  default_model: "meta-llama/llama-3.2-3b-instruct:free"
  thompson_sampling:
    feedback_weight: 0.7        # Higher feedback weight to learn quality differences
    latency_weight: 0.1         # Lower latency priority to save costs
    cost_weight: -0.8           # Maximum cost consideration (very negative)
    exploration_rate: 0.2       # Moderate exploration among free models
  similarity:
    threshold: 0.7
    max_similar_requests: 75    # Good balance of decisions vs speed
    recency_days: 21            # Medium window
    min_similar_requests: 5
  cold_start:
    min_confidence_score: 0.08
    optimistic_prior: 0.75      # Slightly pessimistic to favor proven free models
    exploration_bonus: 0.1
    min_requests_for_global: 15
  persistence:
    batch_size: 150

embedding:
  service_type: "http"
  service_url: "http://localhost:8001"
  model_path: "./models/all-MiniLM-L6-v2"
  max_workers: 4
  cache_size: 1500            # Good cache to avoid re-embedding
  timeout_ms: 4000

database:
  enable_persistence: true    # Enable to learn from history
  host: "localhost"
  port: "5432"
  user: "llm-router"
  name: "llm-router_cost"
  ssl_mode: "disable"
  workers: 5
  buffer_size: 1000

circuit_breaker:
  enabled: true
  failure_threshold: 5
  success_threshold: 2
  timeout: 60s
  max_requests: 3

logging:
  level: "info"
  format: "auto"
  report_caller: false